import numpy as np
import glob
import pandas as pd
import librosa
import matplotlib.pyplot as plt
import os
import csv
from pathlib import Path
from cross_correlation import cross_corr
from butterworth_filter import filterA, butter_filter

class ExperimentEvaluator:
    """Class to preprocess the recordings of the DPOAE experiment.
    Files are sorted to their respective conditions.
    Distortion products are analysed via cross-correlation approach.
    
    Parameters
    ----------
    subject : str
        The subject ID of the experiment
    main_path : str
        The path to the parent folder of the experiment data.

    Attributes
    ----------
    subject : str
        The subject ID of the experiment
    main_path : str
        The path to the parent folder of the experiment data
    exp_path : str
        The path to the experiment data subfolder
    res_path : str
        The path to the results subfolder where DPOAE analysis results are saved
    sig_border : int
        The percentile for significance of the cross-correlation results
    pref : float
        The reference pressure in Pa for sound level calculations
    fs : int
        The sampling frequency of the recordings
    gain : int
        The microphone gain in dB
    sens_mV : float
        The microphone sensitivity in V/Pa
    n_F_res : int
        The harmonic number of the expected DPOAE for female resolved stimuli
    n_F_unres : int
        The harmonic number of the expected DPOAE for female unresolved stimuli
    n_M_res : int
        The harmonic number of the expected DPOAE for male resolved stimuli
    n_M_unres : int
        The harmonic number of the expected DPOAE for male unresolved stimuli
    pt_dp : int
        The pure tone distortion product in Hz, resulting from f1=1kHz, f2=1.5kHz via 2*f1-f2
    """

    def __init__(self, subject, main_path):
        self.subject = subject
        self.main_path = main_path
        self.exp_path = main_path + f'/Recordings/data_{subject}'
        self.res_path = main_path + f'/results_{subject}'
        if not os.path.exists(self.res_path):
            os.makedirs(self.res_path)

        self.sig_border = 97 
        self.pref = 20e-6 

        self.fs = 44100 
        self.gain = 40 
        self.sens_mV = 50e-3 

        self.n_F_res = 5 # female resolved stimuli: n1=7, n2=9
        self.n_F_unres = 12 # female unresolved stimuli: n1=15, n2=18
        self.n_M_res = 4 # male resolved stimuli: n1=6, n2=8
        self.n_M_unres = 12 # male unresolved stimuli: n1=15, n2=18

        self.pt_dp = 800 # pure tone DP

    def load_experiment_info(self, speaker_type, loop_reps=6):
        """Load the experiment information from the excel file generated by PsychoPy.

        Parameters
        ----------
        speaker_type : str
            Can be either 'Single' or 'Competing'
        loop_reps : int
            The number of loop repetitions within the experiment

        Returns
        -------
        excel_info : DataFrame
            The experiment information that was saved during the experiment
        """
        excel_file = glob.glob(self.exp_path + f'/{self.subject}_*.xlsx')
        if len(excel_file) == 1:
            excel_path = excel_file[0]
        else:
            raise ValueError(f'Found {len(excel_file)} excel files for {self.subject}!')
        
        if speaker_type == 'Single':
            excel_info = pd.read_excel(excel_path, sheet_name='loop_SingleSpeaker')
            excel_info = excel_info.iloc[:1, :]
        elif speaker_type == 'Competing':
            excel_info = pd.read_excel(excel_path, sheet_name='loop_Attention') 
            excel_info = excel_info.iloc[:1, :]
            for i in range(1, loop_reps):
                new_sheet = pd.read_excel(excel_path, sheet_name=f'loop_Attention{i}')
                new_sheet = new_sheet.iloc[:1, :]
                excel_info = pd.concat([excel_info, new_sheet], ignore_index=True)
        else:
            raise ValueError('Invalid speaker type! Can either be \'Single\' or \'Competing\'.')

        excel_info.columns = self.clean_header(excel_info.columns)
        
        return excel_info
    
    def clean_header(self, columns):
        """Replaces unnamed header columns with the name of the previous column.

        Parameters
        ----------
        columns : list
            The columns of the dataframe
        
        Returns
        -------
        cleaned_header : list
            The cleaned header of the dataframe
        """
        cleaned_header = []
        current_header = None
        for col in columns:
            if "Unnamed" in col:
                cleaned_header.append(current_header)
            else:
                current_header = col
                cleaned_header.append(current_header)
        return cleaned_header

    def estimate_level(self, x, A_weighting=True):
        """Estimates the sound level of a signal x in dBA.

        Parameters
        ----------
        x : array 
            The signal to be analyzed
        A_weighting : bool
            Whether A-weighting should be applied. By default, this is set to True.

        Returns
        -------
        dB : float
            The estimated sound level in dB SPL or dBA depending on the applied weighting.
        """
        x = x / self.sens_mV # convert to Pa

        if A_weighting:
            fftx = np.abs(np.fft.fft(x))
            fftx[fftx == 0] = 1e-17
            f = (self.fs/len(fftx)) * np.arange(len(fftx))
            ind = np.where(f < self.fs/2)  
            f = f[ind]
            fftx = fftx[ind]
            A = filterA(f)
            fftx = fftx * A
            x = np.real(np.fft.ifft(fftx))
            
        ms = np.mean(np.square(x))
        rms = np.sqrt(ms)
        dB = 20 * np.log10(rms / self.pref) - self.gain
        
        return dB

    def get_given_answers(self, excel_info):
        """Extract the given answers to the comprehension questions from the excel file generated by PsychoPy.

        Parameters
        ----------
        excel_info : DataFrame
            The experiment information, can be loaded via load_experiment_info()

        Returns
        -------
        answers : list
            The given answers to the comprehension questions in the form [[Q1], [Q2], [Q3]].
            If more than three questions were asked, adjust the range in the for loop.
        """
        answers = []
        for i in range(1, 4): # adjust the range if more than three questions were asked
            try:
                no_keys = excel_info.loc[:, f'slider_Q{i}.response_raw'].shape[0]
                sub_answers = []
                for j in range(no_keys):
                    answer = excel_info.loc[j, f'slider_Q{i}.response_raw'].tolist()
                    answer = ['c' if item == 1 else 'b' if item == 2 else 'a' for item in answer]
                    sub_answers.extend(answer)
                answers.append(sub_answers)

            except:
                no_keys = excel_info.loc[:, f'key_Q{i}.keys_raw'].shape[0]
                sub_answers = []
                for j in range(no_keys):
                    answer = excel_info.loc[j, f'key_Q{i}.keys_raw'].tolist()
                    answer = [item.strip().strip("'") for item in answer]
                    sub_answers.extend(answer)
                answers.append(sub_answers)
        return answers # contains answers to [[Q1], [Q2], [Q3]]

    def load_target_info(self, speaker_type):
        """Load the target information from the csv file.

        Parameters
        ----------
        speaker_type : str
            Can be either 'Single' or 'Competing'

        Returns
        -------
        target_info : DataFrame
            The target information that was saved during the experiment
        """
        if speaker_type == 'Single':
            target_info = pd.read_csv(self.exp_path + f'/{self.subject}_targets_Singlespeaker.csv', sep='\t')
        elif speaker_type == 'Competing':
            target_info = pd.read_csv(self.exp_path + f'/{self.subject}_targets_Attention.csv', sep='\t')
        return target_info

    def get_correct_answers(self, target_info):
        """Extract the correct answers to the comprehension questions from the target information.

        Parameters
        ----------
        target_info : DataFrame
            The target information, can be loaded via load_target_info()

        Returns
        -------
        answers : list
            The correct answers to the comprehension questions in the form [[Q1], [Q2], [Q3]].
            If more than three questions were asked, adjust the range in the for loop.
        """
        answers = []
        for i in range(1, 4): # adjust the range if more than three questions were asked
            answer = target_info.loc[:, f'Correct A{i}'].tolist()
            answer = [item.strip().strip("'") for item in answer]
            answers.append(answer)
        return answers # contains answers to [[Q1], [Q2], [Q3]]
    
    def compare_answers(self, given, correct):
        """Compare the given answers to the correct answers.

        Parameters
        ----------
        given : list
            The given answers to the comprehension questions in the form [[Q1], [Q2], [Q3]]
        correct : list
            The correct answers to the comprehension questions in the form [[Q1], [Q2], [Q3]]
        
        Returns
        -------
        correct_answers : list
            Contains 1 if the answer was correct and 0 if it was wrong in the form [[Q1], [Q2], [Q3]]
        """
        correct_answers = []
        for i in range(3): # adjust the range if more than three questions were asked
            correct_answers.append([1 if given[i][j] == correct[i][j] else 0 for j in range(len(given[i]))])
        return correct_answers

    def get_listening_effort(self, excel_info):
        """Extract the listening effort from the excel file generated by PsychoPy.

        Parameters
        ----------
        excel_info : DataFrame
            The experiment information, can be loaded via load_experiment_info()

        Returns
        -------
        listening_effort : list
            The listening effort values
        """
        listening_effort = excel_info.loc[:, 'slider.response_raw'].to_numpy().flatten().tolist()
        return listening_effort
    
    def load_dp_waveforms(self, target, chapter, speaker_type, stim, return_dp_path=False):
        """Load the waveforms of the expected distortion products for the attended and ignored speaker for different targets.

        Parameters
        ----------
        target : str
            The target determined for each chapter (male, female or video)
        chapter : int
            The chapter of the book
        speaker_type : str
            Can be either 'Single' or 'Competing'
        stim : str
            The path to the stimulus for the selected chapter
        return_dp_path : bool
            If True, load_dp_waveforms returns the path to the matching dp wavform instead of the waveform itself
        
        Returns
        -------
        stim_name : str
            Contains the pairing of speaker and audiobook
        wf_dp_attended : array
            The waveforms of the distortion products for the attended speaker in the form [resolved, unresolved]
        wf_dp_ignored : array
            The waveforms of the distortion products for the ignored speaker in the form [resolved, unresolved]
        dp_path_atd : list
            The paths to the distortion product waveforms for the attended speaker in the form [resolved, unresolved]
        dp_path_ign : list
            The paths to the distortion product waveforms for the ignored speaker in the form [resolved, unresolved]
        """
        chapter = int(chapter)

        if speaker_type == 'Single':
            if chapter > 2:
                raise ValueError('Chapter > 2 not supported for Single Speaker!')
            else:
                ch_no = f'0{chapter}'

            if target == 'female':
                dp_path_atd = [f'Stimuli/Polarnacht_female/dp_n{self.n_F_res}_Polarnacht_female_single_{ch_no}.wav', 
                               f'Stimuli/Polarnacht_female/dp_n{self.n_F_unres}_Polarnacht_female_single_{ch_no}.wav']
                dp_path_ign = [f'Stimuli/Darum_male/dp_n{self.n_M_res}_Darum_male_{ch_no}.wav',
                               f'Stimuli/Darum_male/dp_n{self.n_M_unres}_Darum_male_{ch_no}.wav']

                dp_attended_resolved, _ = librosa.load(self.main_path + dp_path_atd[0], sr=self.fs, mono=True)
                dp_attended_unresolved, _ = librosa.load(self.main_path + dp_path_atd[1], sr=self.fs, mono=True)

                # hier gibt es keine passenden waveforms, da bei Single Speaker Darum nicht vorgelesen wird. Zu Demonstrationszwecken kann man aber trotzdem einmal Korrelation von Single Speaker mit beliebigem Darum Ausschnitt berechnen.
                dp_ignored_resolved, _ = librosa.load(self.main_path + dp_path_ign[0], sr=self.fs, mono=True) 
                dp_ignored_unresolved, _ = librosa.load(self.main_path + dp_path_ign[1], sr=self.fs, mono=True)

                stim_name = 'Polarnacht_female'

            elif target == 'male':
                dp_path_atd = [f'Stimuli/Polarnacht_male/dp_n{self.n_M_res}_Polarnacht_male_single_{ch_no}.wav',
                               f'Stimuli/Polarnacht_male/dp_n{self.n_M_unres}_Polarnacht_male_single_{ch_no}.wav']
                dp_path_ign = [f'Stimuli/Darum_female/dp_n{self.n_F_res}_Darum_female_{ch_no}.wav',
                               f'Stimuli/Darum_female/dp_n{self.n_F_unres}_Darum_female_{ch_no}.wav']
                
                dp_attended_resolved, _ = librosa.load(self.main_path + dp_path_atd[0], sr=self.fs, mono=True)
                dp_attended_unresolved, _ = librosa.load(self.main_path + dp_path_atd[1], sr=self.fs, mono=True)

                dp_ignored_resolved, _ = librosa.load(self.main_path + dp_path_ign[0], sr=self.fs, mono=True)
                dp_ignored_unresolved, _ = librosa.load(self.main_path + dp_path_ign[1], sr=self.fs, mono=True)

                stim_name = 'Polarnacht_male'

            else:
                raise ValueError('Could not determine attended and ignored speaker for Single Speaker!')

        elif speaker_type == 'Competing':
            if chapter < 10:
                ch_no = f'0{chapter}'
            else:
                ch_no = str(chapter)

            if target == 'female': # Polarnacht_female attended, Darum_male ignored
                dp_path_atd = [f'Stimuli/Polarnacht_female/dp_n{self.n_F_res}_Polarnacht_female_{ch_no}.wav',
                               f'Stimuli/Polarnacht_female/dp_n{self.n_F_unres}_Polarnacht_female_{ch_no}.wav']
                dp_path_ign = [f'Stimuli/Darum_male/dp_n{self.n_M_res}_Darum_male_{ch_no}.wav',
                               f'Stimuli/Darum_male/dp_n{self.n_M_unres}_Darum_male_{ch_no}.wav']

                dp_attended_resolved, _ = librosa.load(self.main_path + dp_path_atd[0], sr=self.fs, mono=True)
                dp_attended_unresolved, _ = librosa.load(self.main_path + dp_path_atd[1], sr=self.fs, mono=True)

                dp_ignored_resolved, _ = librosa.load(self.main_path + dp_path_ign[0], sr=self.fs, mono=True)
                dp_ignored_unresolved, _ = librosa.load(self.main_path + dp_path_ign[1], sr=self.fs, mono=True)

                stim_name = 'Stimuli_Polarnacht_female_Darum_male'
            
            elif target == 'male': # Polarnacht_male attended, Darum_female ignored
                dp_path_atd = [f'Stimuli/Polarnacht_male/dp_n{self.n_M_res}_Polarnacht_male_{ch_no}.wav',
                               f'Stimuli/Polarnacht_male/dp_n{self.n_M_unres}_Polarnacht_male_{ch_no}.wav']
                dp_path_ign = [f'Stimuli/Darum_female/dp_n{self.n_F_res}_Darum_female_{ch_no}.wav',
                               f'Stimuli/Darum_female/dp_n{self.n_F_unres}_Darum_female_{ch_no}.wav']

                dp_attended_resolved, _ = librosa.load(self.main_path + dp_path_atd[0], sr=self.fs, mono=True)
                dp_attended_unresolved, _ = librosa.load(self.main_path + dp_path_atd[1], sr=self.fs, mono=True)

                dp_ignored_resolved, _ = librosa.load(self.main_path + dp_path_ign[0], sr=self.fs, mono=True)
                dp_ignored_unresolved, _ = librosa.load(self.main_path + dp_path_ign[1], sr=self.fs, mono=True)

                stim_name = 'Stimuli_Polarnacht_male_Darum_female'
            
            elif target == 'video':
                if 'MixedTwobandStimuli_Polarnacht_female_Darum_male' in stim: # Polarnacht_female ignored, Darum_male ignored -> Polarnacht always matches the 'attended' conditions, Darum always matches the 'ignored' conditions
                    dp_path_atd = [f'Stimuli/Polarnacht_female/dp_n{self.n_F_res}_Polarnacht_female_{ch_no}.wav',
                                   f'Stimuli/Polarnacht_female/dp_n{self.n_F_unres}_Polarnacht_female_{ch_no}.wav']
                    dp_path_ign = [f'Stimuli/Darum_male/dp_n{self.n_M_res}_Darum_male_{ch_no}.wav',
                                   f'Stimuli/Darum_male/dp_n{self.n_M_unres}_Darum_male_{ch_no}.wav']

                    dp_attended_resolved, _ = librosa.load(self.main_path + dp_path_atd[0], sr=self.fs, mono=True) # attended refers to Polarnacht waveforms
                    dp_attended_unresolved, _ = librosa.load(self.main_path + dp_path_atd[1], sr=self.fs, mono=True)

                    dp_ignored_resolved, _ = librosa.load(self.main_path + dp_path_ign[0], sr=self.fs, mono=True) # ignored refers to Darum waveforms
                    dp_ignored_unresolved, _ = librosa.load(self.main_path + dp_path_ign[1], sr=self.fs, mono=True)

                    stim_name = 'Stimuli_Polarnacht_female_Darum_male'
                
                elif 'MixedTwobandStimuli_Darum_female_Polarnacht_male' in stim: # Polarnacht_male ignored, Darum_female ignored
                    dp_path_atd = [f'Stimuli/Polarnacht_male/dp_n{self.n_M_res}_Polarnacht_male_{ch_no}.wav',
                                   f'Stimuli/Polarnacht_male/dp_n{self.n_M_unres}_Polarnacht_male_{ch_no}.wav']
                    dp_path_ign = [f'Stimuli/Darum_female/dp_n{self.n_F_res}_Darum_female_{ch_no}.wav',
                                   f'Stimuli/Darum_female/dp_n{self.n_F_unres}_Darum_female_{ch_no}.wav']

                    dp_attended_resolved, _ = librosa.load(self.main_path + dp_path_atd[0], sr=self.fs, mono=True)
                    dp_attended_unresolved, _ = librosa.load(self.main_path + dp_path_atd[1], sr=self.fs, mono=True)

                    dp_ignored_resolved, _ = librosa.load(self.main_path + dp_path_ign[0], sr=self.fs, mono=True)
                    dp_ignored_unresolved, _ = librosa.load(self.main_path + dp_path_ign[1], sr=self.fs, mono=True)

                    stim_name = 'Stimuli_Polarnacht_male_Darum_female'

            else:
                raise ValueError('Could not determine attended and ignored speaker for Competing Speaker!')
            
        else:
            raise ValueError('Invalid speaker type! Can either be \'Single\' or \'Competing\'.')
        
        if return_dp_path:
            return stim_name, dp_path_atd, dp_path_ign
        
        else:
            wf_dp_attended = np.vstack((dp_attended_resolved, dp_attended_unresolved)) # contains waveforms for resolved and unresolved as [resolved, unresolved]
            wf_dp_ignored = np.vstack((dp_ignored_resolved, dp_ignored_unresolved))

            return stim_name, wf_dp_attended, wf_dp_ignored # attended is always Polarnacht, ignored is always Darum
             
    def get_recordings(self, excel_info, speaker_type):
        """Load the recordings of the stimuli (left ear)

        Parameters
        ----------
        excel_info : DataFrame
            The experiment information, can be loaded via load_experiment_info()
        speaker_type : str
            Can be either 'Single' or 'Competing'

        Returns
        -------
        oae_recs : list
            The recordings of the stimuli
        """
        if speaker_type == 'Single':
            col_name = '_SingleSpeaker.clip_raw'
        elif speaker_type == 'Competing':
            col_name = '_Attention.clip_raw'
        else:
            raise ValueError('Invalid speaker type!')
        
        no_recs = excel_info.loc[:, f'mic_stim{col_name}'].shape[0]
        oae_paths = []

        for j in range(no_recs):
            oae = excel_info.loc[j, f'mic_stim{col_name}'].tolist()
            oae_paths.extend(oae)
        oae_recs = []
        
        for i in range(len(oae_paths)):
            oae_paths[i] = oae_paths[i].strip().strip("'")

            if oae_paths[i].startswith("WindowsPath("): # remove WindowsPath wrapper
                clean_oae = Path(oae_paths[i][len("WindowsPath('"):-2])
            else:
                clean_oae = Path(oae_paths[i]) 

            if "data" in clean_oae.parts: # 
                idx = clean_oae.parts.index("data")
                final_oae = Path(self.exp_path) / Path(*clean_oae.parts[idx+1:])
            else:
                final_oae = Path(self.exp_path) / clean_oae

            orec = librosa.load(final_oae, sr=self.fs, mono=True)[0]

            if np.mean(orec) != 0:
                orec = orec - np.mean(orec)
        
            oae_recs.append(orec)

        return oae_recs
    
    def calculate_distortion_product(self, dp, rec, eq_delay, save_path=None, figure_format='.png'):
        """Calculate the distortion product of the DP waveforms and the recordings for resolved and unresolved stimuli.

        Parameters
        ----------
        dp : array
            The distortion product waveforms for resolved and unresolved stimuli, as returned by load_dp_waveforms()
        rec : array
            The recording of the DPOAE
        eq_delay : int
            The equipment delay in frames
        save_path : str
            The path where the results should be saved. If None, the results are not saved.
        figure_format : str
            The format of the figures, can be either '.png' or '.svg'

        Returns
        -------
        stats_resolved : list
            Contains [lag_peak, mag_peak [a.u.], SNR [dB], significance of peak [boolean]] of the resolved DP
        stats_unresolved : list
            Contains [lag_peak, mag_peak [a.u.], SNR [dB], significance of peak [boolean]] of the unresolved DP
        """
        eq_delay = int(eq_delay)
        len_dp = dp.shape[1]
        len_rec = rec.shape[0]

        if len_dp > len_rec:
                dp = dp[:, :len_rec]
        elif len_dp < len_rec:
                rec = rec[:len_dp]

        dp_resolved = dp[0]
        dp_unresolved = dp[1]

        total_length = len(dp_resolved)
        assert total_length == len(rec), 'Length of DP and recording do not match!'

        lag_resolved, real_resolved, imag_resolved, env_resolved = cross_corr(rec, dp_resolved, fs=self.fs)
        lag_unresolved, real_unresolved, imag_unresolved, env_unresolved = cross_corr(rec, dp_unresolved, fs=self.fs)

        env_resolved = self.smooth_envelope(real_resolved, env_resolved)
        env_unresolved = self.smooth_envelope(real_unresolved, env_unresolved)

        lag_resolved = lag_resolved - eq_delay
        lag_unresolved = lag_unresolved - eq_delay

        lag_resolved, real_resolved, imag_resolved, env_resolved = self.center_DP_corr(lag_resolved, real_resolved, imag_resolved, env_resolved)
        lag_unresolved, real_unresolved, imag_unresolved, env_unresolved = self.center_DP_corr(lag_unresolved, real_unresolved, imag_unresolved, env_unresolved)

        lag_peak_resolved, mag_peak_resolved, sig_resolved, snr_resolved = self.find_significant_peak(lag_resolved, env_resolved)
        lag_peak_unresolved, mag_peak_unresolved, sig_unresolved, snr_unresolved = self.find_significant_peak(lag_unresolved, env_unresolved)

        self.plot_distortion_product(lag_resolved, real_resolved, imag_resolved, env_resolved, lag_peak_resolved, mag_peak_resolved, save_path + 'DP_resolved', figure_format) 
        self.plot_distortion_product(lag_unresolved, real_unresolved, imag_unresolved, env_unresolved, lag_peak_unresolved, mag_peak_unresolved, save_path + 'DP_unresolved', figure_format)

        # save the numpy arrays of the correlations
        save_corr_resolved = np.array([lag_resolved, real_resolved, imag_resolved, env_resolved])
        save_corr_unresolved = np.array([lag_unresolved, real_unresolved, imag_unresolved, env_unresolved])

        np.save(save_path + 'corr_resolved.npy', save_corr_resolved)
        np.save(save_path + 'corr_unresolved.npy', save_corr_unresolved)

        stats_resolved = [lag_peak_resolved, mag_peak_resolved, snr_resolved, sig_resolved]
        stats_unresolved = [lag_peak_unresolved, mag_peak_unresolved, snr_unresolved, sig_unresolved]

        return stats_resolved, stats_unresolved
    
    def center_DP_corr(self, lag, real, imag, env):
        """After compensating for the equipment delay, choose a segment that is symmetrical around 0 to correctly align the data.
        
        Parameters
        ----------
        lag : array
            The lag of the cross-correlation in frames
        real : array
            The real part of the cross-correlation
        imag : array
            The imaginary part of the cross-correlation
        env : array
            The envelope of the cross-correlation

        Returns
        -------
        lag : array
            The centered lag of the cross-correlation in frames
        real : array
            The centered real part of the cross-correlation
        imag : array
            The centered imaginary part of the cross-correlation
        env : array
            The centered envelope of the cross-correlation
        """
        border = 40000 # frames, corresponds to ~900 ms at 44.1 kHz
        idx_min = np.where(lag == -border)[0][0]
        idx_max = np.where(lag == border)[0][0]
        lag = lag[idx_min:idx_max + 1]
        real = real[idx_min:idx_max + 1]
        imag = imag[idx_min:idx_max + 1]
        env = env[idx_min:idx_max + 1]
        return lag, real, imag, env

    def find_significant_peak(self, lag, env, roi=7):
        """Determine if a peak is significant with respect to the noise level

        Parameters
        ----------
        lag : array
            The lag of the cross-correlation in frames
        env : array
            The envelope of the cross-correlation
        roi : int
            The region of interest in ms around the peak
        
        Returns
        -------
        lag_peak : float
            The lag of the peak in frames
        mag_peak : float
            The magnitude of the peak
        sig : bool
            Whether the peak is significant
        snr : float
            The signal-to-noise ratio of the peak in dB
        """
        # find peak amplitude between -roi and roi ms
        border = roi / 1000 * self.fs # in frames
        idx_min = np.argmin(np.abs(lag + border))
        idx_max = np.argmin(np.abs(lag - border))
        env_range = env[idx_min:idx_max + 1]

        mag_peak, idx_peak = np.max(env_range), np.argmax(env_range)
        idx_peak += idx_min
        lag_peak = lag[idx_peak] # in frames

        # determine if peak is significant
        l, h = 70, 750 # ms
        l, h = l / 1000 * self.fs, h / 1000 * self.fs # frames
        idx_noise = np.where(((lag < -l) & (lag > -h)) | ((lag > l) & (lag < h)))[0]
        env_noise = env[idx_noise]

        perc = np.percentile(env_noise, self.sig_border)
        if mag_peak > perc:
            sig = True
        else:
            sig = False

        # determine the snr of the peak in dB
        snr = 20 * np.log10(mag_peak / np.mean(env_noise))

        return lag_peak, mag_peak, sig, snr

    def smooth_envelope(self, carrier, env, cf=8):
        """Smooth the envelope of the cross-correlation by applying a lowpass filter.

        Parameters
        ----------
        carrier : array
            The carrier signal
        env : array
            The envelope of the cross-correlation
        cf : int
            The factor by which the mean frequency of the carrier signal should be divided

        Returns
        -------
        env : array
            The smoothed envelope
        """
        fft_real = np.abs(np.fft.fft(carrier))
        fft_real = fft_real[:len(fft_real)//2] # only take values bigger than 0 in first half of spectrum
        fft_real = fft_real[1:] # exclude 0
        freq_real = np.fft.fftfreq(len(fft_real), 1/self.fs)
        cutoff = freq_real[np.argmax(fft_real)] / cf

        if cutoff > 0:
            env = butter_filter(env, cutoff, 'lowpass')
        
        return env

    def plot_distortion_product(self, lag, corr_real, corr_imag, env, lag_peak, mag, path=None, limit=True, figure_format='.png'):
        """Plot the complex cross-correlation of the DP waveform and the microphone recording.

        Parameters
        ----------
        lag : array
            The lag of the cross-correlation in frames
        corr_real : array
            The real part of the cross-correlation
        corr_imag : array
            The imaginary part of the cross-correlation
        env : array
            The envelope of the cross-correlation
        lag_peak : float
            The lag of the peak in frames
        mag : float
            The magnitude of the peak
        path : str
            The path where the figure should be saved
        limit : bool
            True if the figure should show only the peak region
        figure_format : str
            The format of the figure, can be either '.png' or '.svg'

        Returns
        -------
        None
        """
        disp_border = 30 # ms
        lag_ms = lag / self.fs * 1000 # convert lag to ms
        lag_peak_ms = lag_peak / self.fs * 1000 #convert lag_peak to ms

        plt.figure()
        plt.plot(lag_ms, corr_real, label='real')
        plt.plot(lag_ms, corr_imag, label='imag')
        plt.plot(lag_ms, env, label='envelope', c='k')
        plt.plot(lag_ms, -env, c='k')
        plt.axvline(lag_peak_ms, c='silver', ls='--', label=f'{lag_peak_ms:.2f} ms')
        plt.legend(loc = 'upper right', ncol=2)
        plt.xlabel('Lag [ms]')
        plt.ylabel('Correlation')
        plt.title(f'Correlation of DP waveform vs. microphone recording')
        plt.grid()
        plt.tight_layout()

        if path is not None:
            if figure_format == '.png':
                plt.savefig(path + '_complete.png')
            elif figure_format == '.svg':
                plt.savefig(path + '_complete.svg', format='svg', dpi=1200, transparent=True)
            else:
                raise ValueError('Invalid file format! Must be .png or .svg.')
            
        if limit:
            plt.xlim(-disp_border, disp_border)
            plt.ylim(-2*mag, 2*mag)
            if path is not None:
                if figure_format == '.png':
                    plt.savefig(path + '_limits.png')
                elif figure_format == '.svg':   
                    plt.savefig(path + '_limits.svg', format='svg', dpi=1200, transparent=True)
                else:
                    raise ValueError('Invalid file format! Must be .png or .svg.')
        plt.close()
        return

    def calculate_equipment_delay(self, stimuli, rec, save_path=None, figure_format='.png'):
        """Calculate the equipment delay though the cross-correlation between the microphone recording and the stimuli.

        Parameters
        ----------
        stimuli : list
            The stimuli recordings
        rec : array
            The microphone recording
        save_path : str
            The path where the results should be saved
        figure_format : str
            The format of the figure, can be either '.png' or '.svg'

        Returns
        -------
        eq_delay : float
            The equipment delay in frames
        """
        disp_border = 30 # ms
        delay = np.zeros(2)

        plt.figure(figsize=(10, 7))

        for i, stim in enumerate([stimuli[0], stimuli[1]]):
            if len(stim) > len(rec):
                stim = stim[:len(rec)]
            elif len(stim) < len(rec):
                rec = rec[:len(stim)]

            lag, corr_real, corr_imag, env = cross_corr(rec, stim, fs=self.fs)
            env = self.smooth_envelope(corr_real, env)
            delay[i] = lag[np.argmax(env)] # in frames
            mag = np.max(env)

            # plot the correlations    
            lag = lag - delay[i] # in frames
            lag_ms = lag / self.fs * 1000 # convert lag to ms
            plt.subplot(2, 1, i+1)
            plt.plot(lag_ms, corr_real, label='real')
            plt.plot(lag_ms, corr_imag, label='imag')
            plt.plot(lag_ms, env, label='envelope', c='k')
            plt.plot(lag_ms, -env, c='k')
            plt.legend(loc = 'upper right', ncol=2)
            plt.xlabel('Lag [ms]')
            plt.ylabel('Correlation')
            plt.title(f'Correlation of f{i+1} vs. microphone recording')
            plt.grid()
            plt.xlim(-disp_border, disp_border)
            plt.ylim(-1.3*mag, 1.3*mag)
            plt.tight_layout()

        if save_path is not None:
            if figure_format == '.png':
                plt.savefig(save_path + '.png')
            elif figure_format == '.svg':
                plt.savefig(save_path + '.svg', format='svg', dpi=1200, transparent=True)
            else:
                raise ValueError('Invalid file format! Must be .png or .svg.')
            
        plt.close()
        eq_delay = np.floor(np.mean(delay)) # equipment delay in frames

        return eq_delay

    def write_results_csv(self, columns, results, save_path):
        """Write the results to a csv file

        Parameters
        ----------
        columns : list
            The column names
        results : list
            The results, number of columns must match the number of column names
        save_path : str
            The path where the results should be saved

        Returns
        -------
        None
        """
        with open(save_path, 'w') as f:
            writer = csv.writer(f)
            writer.writerow(columns)
            writer.writerows(results)
        return

    def oae_analysis(self, speaker_type):
        """Analyze the OAEs for the different speaker types.

        Parameters
        ----------
        speaker_type : str
            Can be either 'Single' or 'Competing'

        Returns
        -------
        None
        """
        if speaker_type == 'Single':
            res_path = self.res_path + '/Single_Speaker'
            passing_limit = 1
        elif speaker_type == 'Competing':
            res_path = self.res_path + '/Competing_Speaker'
            passing_limit = 2
        else:
            raise ValueError('Invalid speaker type! Can either be \'Single\' or \'Competing\'.')
        
        if not os.path.exists(res_path):
            os.makedirs(res_path)

        columns_cq = ['Chapter', 'Target', 'Given A1', 'Given A2', 'Given A3', 'Correct A1', 'Correct A2', 'Correct A3', 'Number Correct Answers', 'Listening Effort']
        columns_stats = ['Chapter', 'Target', 'Stimulus Shortcut', 'Number Correct Answers',
                         'ATTENDED Delay [frames] (resolved)', 'ATTENDED Mag [a.u.] (resolved)', 'ATTENDED SNR [dB] (resolved)', 'ATTENDED Significant Peak (resolved)', 
                         'ATTENDED Delay [frames] (unresolved)', 'ATTENDED Mag [a.u.] (unresolved)', 'ATTENDED SNR [dB] (unresolved)', 'ATTENDED Significant Peak (unresolved)',
                         'IGNORED Delay [frames] (resolved)', 'IGNORED Mag [a.u.] (resolved)', 'IGNORED SNR [dB] (resolved)', 'IGNORED Significant Peak (resolved)',
                         'IGNORED Delay [frames] (unresolved)', 'IGNORED Mag [a.u.] (unresolved)', 'IGNORED SNR [dB] (unresolved)', 'IGNORED Significant Peak (unresolved)',
                         'Equipment Delay [frames]', 'Stimulus Path']
        columns_vol = ['Chapter', 'Target', 'Volume Stimulus [dB SPL]']

        excel_info = self.load_experiment_info(speaker_type)
        target_info = self.load_target_info(speaker_type)

        given_answers = self.get_given_answers(excel_info)
        correct_answers = self.get_correct_answers(target_info)
        compared_answers = self.compare_answers(given_answers, correct_answers) # contains answers to [[Q1], [Q2], [Q3]] with len(Q1) = len(Q2) = len(Q3) = len(oae_recs)
        count_correct_answers = np.sum(compared_answers, axis=0)
        listening_effort = self.get_listening_effort(excel_info)

        oae_recs = self.get_recordings(excel_info, speaker_type)

        if speaker_type == 'Single':
            targets = target_info['Speaker']
        elif speaker_type == 'Competing':
            targets = target_info['Target']
        stim_paths = target_info['Stimulus']
        chapters = target_info['Chapter']
        chapters = [int(c) for c in chapters]

        print(f'\nAnalyzing OAEs for {speaker_type} Speaker...')

        list_stats = []
        list_vol = []
        for chapter in chapters:
            print(f'Chapter {chapter}...')
            if chapter < 10:
                ch_no = f'0{chapter}'
            else:
                ch_no = str(chapter)

            target = targets[chapter-1].strip()
            stim_path = stim_paths[chapter-1].strip().replace("\\", "/")

            # determine volume of stimuli (left ear)
            vol_stim = self.estimate_level(oae_recs[chapter-1], A_weighting=False)

            # check the sum of the compared answers -> if < 2, sort results in different folder
            no_correct_answers = count_correct_answers[chapter-1]
            if no_correct_answers < passing_limit: # sort into different folder
                sub_path = res_path + f'/Unsuccessful_attention'
            else:
                sub_path = res_path + f'/Successful_attention'

            stim_name, wf_dp_attended, wf_dp_ignored = self.load_dp_waveforms(target, chapter, speaker_type, stim_path) # attended -> Polarnacht, ignored -> Darum

            if target == 'female':
                save_path = sub_path + '/Target_female'
            elif target == 'male':
                save_path = sub_path + '/Target_male'
            elif target == 'video':
                save_path = sub_path + '/Target_video'
            else:
                raise ValueError('Invalid target!')
            
            save_path_attended = save_path + f'/Attended_DPs'
            save_path_ignored = save_path + f'/Ignored_DPs'

            if not os.path.exists(save_path_attended):
                os.makedirs(save_path_attended)
            if not os.path.exists(save_path_ignored):
                os.makedirs(save_path_ignored)

            stimuli, _ = librosa.load(self.main_path + stim_path, sr=self.fs, mono=False)
            eq_delay = self.calculate_equipment_delay(stimuli, oae_recs[chapter-1], save_path + f'/Ch{ch_no}_eq_delay') 

            # calculate_distortion_product() returns [lag_peak, mag_peak, snr, sig] for resolved and unresolved
            attended_stats_resolved, attended_stats_unresolved = self.calculate_distortion_product(wf_dp_attended, oae_recs[chapter-1], eq_delay, save_path_attended + f'/Ch{ch_no}_', figure_format='.png')
            ignored_stats_resolved, ignored_stats_unresolved = self.calculate_distortion_product(wf_dp_ignored, oae_recs[chapter-1], eq_delay, save_path_ignored + f'/Ch{ch_no}_', figure_format='.png')

            stats = [chapter, target, stim_name, no_correct_answers] + attended_stats_resolved + attended_stats_unresolved + ignored_stats_resolved + ignored_stats_unresolved + [eq_delay, stim_path]
            list_stats.append(stats)
            list_vol.append([chapter, target, vol_stim])
        
        r, c = np.array(given_answers).shape
        list_cq = np.concatenate((np.array(list_stats)[:, :2], np.array(given_answers).T, np.array(correct_answers).T, np.array(count_correct_answers).reshape(c, 1), np.array(listening_effort).reshape(c, 1)), axis=1)
        self.write_results_csv(columns_cq, list_cq, self.res_path + f'/cq_{speaker_type}Speaker.csv')
        self.write_results_csv(columns_stats, list_stats, self.res_path + f'/stats_{speaker_type}Speaker.csv')
        self.write_results_csv(columns_vol, list_vol, self.res_path + f'/vol_{speaker_type}Speaker.csv')
        return 
    
    def extract_min_max_frequency(self, wf, nharm, voice):
        """Extract the minimal and maximal frequency of the waveform using the pyin algorithm.
        
        Parameters
        ----------
        wf : array
            The waveform to be analyzed
        nharm : int
            The harmonic number of the waveform
        voice : str
            Can be either 'male' or 'female' and determines the frequency range of the pyin algorithm
        
        Returns
        -------
        min_freq : float
            The minimal frequency of the waveform
        max_freq : float
            The maximal frequency of the waveform
        """
        fr_length = 4000
        win_length = 3300
        hop_length = 1250
        if voice == 'female':
            low, high = 80, 300
        elif voice == 'male':
            low, high = 50, 200
        freqs, voiced_sec, _ = librosa.pyin(wf, sr=self.fs, fmin=(nharm*low), fmax=(nharm*high), frame_length=fr_length, hop_length=hop_length, win_length=win_length)
        min_freq = min(freqs[voiced_sec])
        max_freq = max(freqs[voiced_sec])
        
        return min_freq, max_freq
    
    def run_evaluation(self):
        """Run the evaluation"""
        self.oae_analysis('Single')
        self.oae_analysis('Competing')
        return



